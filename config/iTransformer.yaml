
# basic config
task_name: long_term_forecast  # 'task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]'
is_training: 1 # '1 for training, 0 for testing'
model_id: test
model: iTransformer

# name
test_name: iTransformer-TCN-MLP-multy

# data loader
data: LOAD
root_path: ./dataset/
data_path: Two_stage_processed_Elia_data_original_for_prediction.csv
features: MS
target: OT  # target feature in S or MS task
freq: m
checkpoints: ./checkpoints/

# forecasting task
seq_len: 96
label_len: 72
pred_len: 24
seasonal_patterns: Monthly
inverse: false

# inputation task
mask_rate: 0.0

# anomaly detection task
anomaly_ratio: 0.25

# model define
use_tcn: True
use_mlp: True
expand: 2 # expansion factor for Mamba
d_conv: 4 # conv kernel size for Mamba
top_k: 5  # for TimesBlock
num_kernels: 6  # for Inception
enc_in: 11 # encoder input size
dec_in: 11 # decoder input size
c_out: 11  # output size
d_model: 64  # dimension of model
n_heads: 8  # num of heads
e_layers: 2 # num of encoder layers
d_layers: 2 # num of decoder layers
d_ff: 64  # dimension of fcn
moving_avg: 25  # window size of moving average
factor: 3 # attn factor
distil: true  # whether to use distilling in encoder, using this argument means not using distilling
dropout: 0.05
embed: timeF  # time features encoding, options:[timeF, fixed, learned]
activation: gelu  
output_attention: false # whether to output attention in ecoder
channel_independence: 1 # 0: channel dependence 1: channel independence for FreTS model
decomp_method: moving_avg # method of series decompsition, only support moving_avg or dft_decomp
use_norm: 1 # whether to use normalize; True 1 False 0
down_sampling_layers: 0 # num of down sampling layers
down_sampling_window: 1 # down sampling window size
down_sampling_method: null  # down sampling method, only support avg, max, conv
seg_len: 48 # the length of segmen-wise iteration of SegRNN

# optimization
num_workers: 20
itr: 1
train_epochs: 15
batch_size: 320
patience: 3
learning_rate: 0.0001
des: Exp
loss: MSE
lradj: type2 # adjust learning rate
use_amp: true

# GPU
use_gpu: true
gpu: 0
use_multi_gpu: false
devices: '0,1,2,3'

# de-stationary projector params
p_hidden_dims: [128, 128] # hidden layer dimensions of projector (List)
p_hidden_layers: 2  # number of hidden layers in projector

# metrics (dtw)
use_dtw: false  # the controller of using dtw metric (dtw is time consuming, not suggested unless necessary)

# augmentation
augmentation_ratio: 0 # How many times to augment
seed: 2 # Randomization seed
jitter: false # Jitter preset augmentation
scaling: false  # Scaling preset augmentation
permutation: false
randompermutation: false
magwarp: false
timewarp: false
windowslice: false
windowwarp: false
rotation: false
spawner: false
dtwwarp: false
shapedtwwarp: false
wdba: false
discdtw: false
discsdtw: false
extra_tag: ""