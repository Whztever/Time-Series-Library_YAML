# changed params
model: TimesNet ##
features: MS
factor: 3 # attn factor
enc_in: 7 # encoder input size
dec_in: 7 # decoder input size
c_out: 1  # output size
d_model: 16  # dimension of model
d_ff: 32  # dimension of fcn
loss: MSE

# basic config
task_name: long_term_forecast  # 'task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]'
is_training: 1 # '1 for training, 0 for testing'
model_id: TimesNet_test
# model: TimesNet ##

# data loader
data: ETTh1
root_path: ./data/
data_path: ETTh1.csv
# features: MS
target: OT  # target feature in S or MS task
freq: m
checkpoints: ./checkpoints/

# forecasting task
seq_len: 96
label_len: 48
pred_len: 96
seasonal_patterns: Monthly
inverse: false

# inputation task
mask_rate: 0.15

# anomaly detection task
anomaly_ratio: 0.25

# model define
expand: 2 # expansion factor for Mamba
d_conv: 4 # conv kernel size for Mamba
top_k: 5  # for TimesBlock
num_kernels: 6  # for Inception
# enc_in: 7 # encoder input size
# dec_in: 7 # decoder input size
# c_out: 7  # output size
# d_model: 512  # dimension of model
n_heads: 8  # num of heads
e_layers: 2 # num of encoder layers
d_layers: 1 # num of decoder layers
# d_ff: 2048  # dimension of fcn
moving_avg: 25  # window size of moving average
# factor: 1 # attn factor
distil: true  # whether to use distilling in encoder, using this argument means not using distilling
dropout: 0.1
embed: timeF  # time features encoding, options:[timeF, fixed, learned]
activation: gelu  
output_attention: false # whether to output attention in ecoder
channel_independence: 1 # 0: channel dependence 1: channel independence for FreTS model
decomp_method: moving_avg # method of series decompsition, only support moving_avg or dft_decomp
use_norm: 1 # whether to use normalize; True 1 False 0
down_sampling_layers: 0 # num of down sampling layers
down_sampling_window: 1 # down sampling window size
down_sampling_method: null  # down sampling method, only support avg, max, conv
seg_len: 48 # the length of segmen-wise iteration of SegRNN

# optimization
num_workers: 15
itr: 1
train_epochs: 10
batch_size: 32
patience: 3
learning_rate: 0.0001
des: Exp  # exp description
# loss: MSE
lradj: type1 # adjust learning rate
use_amp: False

# GPU
use_gpu: true
gpu: 0
use_multi_gpu: false
devices: '0,1,2,3'

# de-stationary projector params
p_hidden_dims: [128, 128] # hidden layer dimensions of projector (List)
p_hidden_layers: 2  # number of hidden layers in projector

# metrics (dtw)
use_dtw: false  # the controller of using dtw metric (dtw is time consuming, not suggested unless necessary)

# augmentation
augmentation_ratio: 0 # How many times to augment
seed: 2 # Randomization seed
jitter: false # Jitter preset augmentation
scaling: false  # Scaling preset augmentation
permutation: false
randompermutation: false
magwarp: false
timewarp: false
windowslice: false
windowwarp: false
rotation: false
spawner: false
dtwwarp: false
shapedtwwarp: false
wdba: false
discdtw: false
discsdtw: false
extra_tag: ""